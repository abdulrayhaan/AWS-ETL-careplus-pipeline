{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69535b6-6f3d-414d-9a2f-1b7079ff4185",
   "metadata": {},
   "source": [
    "## Glue Script to transform raw .csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b80dfa-d3f0-4b89-8d20-861b7a0c0657",
   "metadata": {},
   "source": [
    "Below glue script is used to transform raw .csv files and then load them to support-tickets/processed/\n",
    "This will automatically triggered when you use lambda function automate_support_tickets mentioned in lambda folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c5924-1f14-4025-902c-f8cd1756d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.gluetypes import *\n",
    "from awsgluedq.transforms import EvaluateDataQuality\n",
    "from awsglue import DynamicFrame\n",
    "import re\n",
    "\n",
    "def _find_null_fields(ctx, schema, path, output, nullStringSet, nullIntegerSet, frame):\n",
    "    if isinstance(schema, StructType):\n",
    "        for field in schema:\n",
    "            new_path = path + \".\" if path != \"\" else path\n",
    "            output = _find_null_fields(ctx, field.dataType, new_path + field.name, output, nullStringSet, nullIntegerSet, frame)\n",
    "    elif isinstance(schema, ArrayType):\n",
    "        if isinstance(schema.elementType, StructType):\n",
    "            output = _find_null_fields(ctx, schema.elementType, path, output, nullStringSet, nullIntegerSet, frame)\n",
    "    elif isinstance(schema, NullType):\n",
    "        output.append(path)\n",
    "    else:\n",
    "        x, distinct_set = frame.toDF(), set()\n",
    "        for i in x.select(path).distinct().collect():\n",
    "            distinct_ = i[path.split('.')[-1]]\n",
    "            if isinstance(distinct_, list):\n",
    "                distinct_set |= set([item.strip() if isinstance(item, str) else item for item in distinct_])\n",
    "            elif isinstance(distinct_, str) :\n",
    "                distinct_set.add(distinct_.strip())\n",
    "            else:\n",
    "                distinct_set.add(distinct_)\n",
    "        if isinstance(schema, StringType):\n",
    "            if distinct_set.issubset(nullStringSet):\n",
    "                output.append(path)\n",
    "        elif isinstance(schema, IntegerType) or isinstance(schema, LongType) or isinstance(schema, DoubleType):\n",
    "            if distinct_set.issubset(nullIntegerSet):\n",
    "                output.append(path)\n",
    "    return output\n",
    "\n",
    "def drop_nulls(glueContext, frame, nullStringSet, nullIntegerSet, transformation_ctx) -> DynamicFrame:\n",
    "    nullColumns = _find_null_fields(frame.glue_ctx, frame.schema(), \"\", [], nullStringSet, nullIntegerSet, frame)\n",
    "    return DropFields.apply(frame=frame, paths=nullColumns, transformation_ctx=transformation_ctx)\n",
    "\n",
    "def sparkSqlQuery(glueContext, query, mapping, transformation_ctx) -> DynamicFrame:\n",
    "    for alias, frame in mapping.items():\n",
    "        frame.toDF().createOrReplaceTempView(alias)\n",
    "    result = spark.sql(query)\n",
    "    return DynamicFrame.fromDF(result, glueContext, transformation_ctx)\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME','input_file_path'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Default ruleset used by all target nodes with data quality enabled\n",
    "DEFAULT_DATA_QUALITY_RULESET = \"\"\"\n",
    "    Rules = [\n",
    "        ColumnCount > 0\n",
    "    ]\n",
    "\"\"\"\n",
    "\n",
    "# Script generated for node Amazon S3\n",
    "AmazonS3_node1756804441863 = glueContext.create_dynamic_frame.from_options(format_options={\"quoteChar\": \"\\\"\", \"withHeader\": True, \"separator\": \",\", \"optimizePerformance\": False}, connection_type=\"s3\", format=\"csv\", connection_options={\"paths\": [args['input_file_path']], \"recurse\": True}, transformation_ctx=\"AmazonS3_node1756804441863\")\n",
    "\n",
    "# Script generated for node Change Schema\n",
    "ChangeSchema_node1756804913687 = ApplyMapping.apply(frame=AmazonS3_node1756804441863, mappings=[(\"ticket_id\", \"string\", \"ticket_id\", \"string\"), (\"created_at\", \"string\", \"created_at\", \"timestamp\"), (\"resolved_at\", \"string\", \"resolved_at\", \"timestamp\"), (\"agent\", \"string\", \"agent\", \"string\"), (\"priority\", \"string\", \"priority\", \"string\"), (\"num_interactions\", \"string\", \"num_interactions\", \"int\"), (\"issuecat\", \"string\", \"issuecat\", \"string\"), (\"channel\", \"string\", \"channel\", \"string\"), (\"status\", \"string\", \"status\", \"string\"), (\"agent_feedback\", \"string\", \"agent_feedback\", \"string\")], transformation_ctx=\"ChangeSchema_node1756804913687\")\n",
    "\n",
    "# Script generated for node Drop Null Fields\n",
    "DropNullFields_node1756805170387 = drop_nulls(glueContext, frame=ChangeSchema_node1756804913687, nullStringSet={\"\"}, nullIntegerSet={}, transformation_ctx=\"DropNullFields_node1756805170387\")\n",
    "\n",
    "# Script generated for node Rename Field - Issue Category\n",
    "RenameFieldIssueCategory_node1756805246468 = RenameField.apply(frame=DropNullFields_node1756805170387, old_name=\"issuecat\", new_name=\"issue_category\", transformation_ctx=\"RenameFieldIssueCategory_node1756805246468\")\n",
    "\n",
    "# Script generated for node Filter - Number of iterations\n",
    "FilterNumberofiterations_node1756805327707 = Filter.apply(frame=RenameFieldIssueCategory_node1756805246468, f=lambda row: (row[\"num_interactions\"] >= 0), transformation_ctx=\"FilterNumberofiterations_node1756805327707\")\n",
    "\n",
    "# Script generated for node SQL Query - Priority\n",
    "SqlQuery94 = '''\n",
    "select *,\n",
    "case\n",
    "    when priority=\"Lw\" then \"Low\"\n",
    "    when priority=\"Medum\" then \"Medium\"\n",
    "    when priority=\"Hgh\" then \"High\"\n",
    "else priority\n",
    "end as priority\n",
    "from myDataSource\n",
    "'''\n",
    "SQLQueryPriority_node1756805413536 = sparkSqlQuery(glueContext, query = SqlQuery94, mapping = {\"myDataSource\":FilterNumberofiterations_node1756805327707}, transformation_ctx = \"SQLQueryPriority_node1756805413536\")\n",
    "\n",
    "# Script generated for node Select Fields\n",
    "SelectFields_node1756805920995 = SelectFields.apply(frame=SQLQueryPriority_node1756805413536, paths=[\"ticket_id\", \"created_at\", \"resolved_at\", \"agent\", \"priority\", \"num_interactions\", \"issue_category\", \"channel\", \"status\"], transformation_ctx=\"SelectFields_node1756805920995\")\n",
    "\n",
    "# Script generated for node Amazon S3\n",
    "EvaluateDataQuality().process_rows(frame=SelectFields_node1756805920995, ruleset=DEFAULT_DATA_QUALITY_RULESET, publishing_options={\"dataQualityEvaluationContext\": \"EvaluateDataQuality_node1756804431571\", \"enableDataQualityResultsPublishing\": True}, additional_options={\"dataQualityResultsPublishing.strategy\": \"BEST_EFFORT\", \"observations.scope\": \"ALL\"})\n",
    "if (SelectFields_node1756805920995.count() >= 1):\n",
    "   SelectFields_node1756805920995 = SelectFields_node1756805920995.coalesce(1)\n",
    "AmazonS3_node1756806083039 = glueContext.write_dynamic_frame.from_options(frame=SelectFields_node1756805920995, connection_type=\"s3\", format=\"glueparquet\", connection_options={\"path\": \"s3://my-careplus-data/support-tickets/processed/\", \"partitionKeys\": []}, format_options={\"compression\": \"snappy\"}, transformation_ctx=\"AmazonS3_node1756806083039\")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b87af-c303-4188-a7c7-bc0111947e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
