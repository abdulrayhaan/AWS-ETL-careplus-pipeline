{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c898978-9b1c-43f9-b515-a4a588682c16",
   "metadata": {},
   "source": [
    "# Lambda Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab3f87-a042-4f8b-99c2-63b6c6cebc09",
   "metadata": {},
   "source": [
    "## automate_support_logs_etl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f0b1f4-5d76-4473-83bf-f911b68d1792",
   "metadata": {},
   "source": [
    "Below includes data transformation of log data as well.\n",
    "This code is to convert raw .log files to processed .parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e023f-bb5c-41eb-be59-67e86030c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import re\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "\n",
    "def save_parquet_to_s3(df, bucket, key):\n",
    "    # Convert DataFrame to Parquet\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    pq.write_table(table, parquet_buffer)\n",
    "\n",
    "    # Upload to S3\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=parquet_buffer.getvalue())\n",
    "    print(f\"‚úÖ Parquet saved to s3://{bucket}/{key}\")\n",
    "    \n",
    "\n",
    "def read_log_from_s3(bucket, key):\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    log_data = response['Body'].read().decode('utf-8')\n",
    "    return log_data\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # 1: read data from the bucket\n",
    "    # Get bucket and object key from the S3 event trigger\n",
    "    record = event['Records'][0]\n",
    "    bucket_name = record['s3']['bucket']['name']\n",
    "    input_key = record['s3']['object']['key']\n",
    "\n",
    "    print(f\"üì• Triggered by: s3://{bucket_name}/{input_key}\")\n",
    "\n",
    "    # Step 2: Read log data\n",
    "    raw_logs = read_log_from_s3(bucket_name, input_key)\n",
    "\n",
    "    # Split the log entries using the delimiter\n",
    "    entries = [entry.strip() for entry in raw_logs.split('---') if entry.strip()]\n",
    "\n",
    "    # Regex pattern to extract data\n",
    "    log_pattern = re.compile(\n",
    "        r'(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) \\[(?P<log_level>[A-Za-z0-9_]+)\\] '\n",
    "        r'(?P<component>[^\\s]+) - TicketID=(?P<ticket_id>[^\\s]+) SessionID=(?P<session_id>[^\\s]+)\\s*'\n",
    "        r'IP=(?P<ip>.*?) \\| ResponseTime=(?P<response_time>-?\\d+)ms \\| CPU=(?P<cpu>[\\d.]+)% \\| EventType=(?P<event_type>.*?) \\| Error=(?P<error>\\w+)\\s*'\n",
    "        r'UserAgent=\"(?P<user_agent>.*?)\"\\s*'\n",
    "        r'Message=\"(?P<message>.*?)\"\\s*'\n",
    "        r'Debug=\"(?P<debug>.*?)\"\\s*'\n",
    "        r'TraceID=(?P<trace_id>.*)'\n",
    "    )\n",
    "\n",
    "    # Extract structured data\n",
    "    parsed_entries = []\n",
    "    for entry in entries:\n",
    "        match = log_pattern.search(entry)\n",
    "        if match:\n",
    "            parsed_entries.append(match.groupdict())\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(parsed_entries)\n",
    "\n",
    "    # Data cleaning\n",
    "    # i) Drop trace_id column\n",
    "    df = df.drop('trace_id', axis=1)\n",
    "\n",
    "    # ii) Remove Negative response time\n",
    "    df = df[df['response_time'].astype(int) >= 0]\n",
    "\n",
    "    # iii) typo-fix in log_level \n",
    "    fix_log_level = {'INF0': 'INFO', 'DEBG': 'DEBUG', 'warnING': 'WARNING', 'EROR': 'ERROR'}\n",
    "    df['log_level'] = df['log_level'].replace(fix_log_level)\n",
    "\n",
    "    # iv) Remove duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # v) Change to appropriate data types (response_time, cpu, timestamp, error)\n",
    "    df['response_time'] = df['response_time'].astype(int)\n",
    "    df['cpu'] = df['cpu'].astype(float)\n",
    "    df['error'] = df['error'].str.lower().map({'true': True, 'false': False})\n",
    "\n",
    "    # timestamp conversion\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S', errors='coerce').astype('datetime64[ms]')\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "\n",
    "    # Save the data (Upload Parquet to S3)\n",
    "    output_file_name = input_key.split('/')[2].replace('.log', '.parquet')\n",
    "    output_key = f'support-logs/processed/{output_file_name}'\n",
    "    save_parquet_to_s3(df, bucket_name, output_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df30d6b-92d3-4625-a5d4-dbd625d26f46",
   "metadata": {},
   "source": [
    "## automate_support_tickets_etl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347fb204-a8bb-4eda-8d4d-39f72fbd11f8",
   "metadata": {},
   "source": [
    "Transformation for csv files is done in AWS Glue , you can find glue script in glue file below, this code is just automate transformation and load.\n",
    "This code is to convert raw .csv files to processed .parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65c94e-4c97-4320-a592-6832d184fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "glue = boto3.client('glue',region_name='<your region>')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Get the S3 file path\n",
    "    bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    input_key = event['Records'][0]['s3']['object']['key']\n",
    "    s3_input_path = f's3://{bucket}/{input_key}'\n",
    "\n",
    "    print(f\"Triggering Glue job with file: {s3_input_path}\")\n",
    "\n",
    "    response = glue.start_job_run(\n",
    "        JobName='ETL_support_tickets-copy',  \n",
    "        Arguments={\n",
    "            '--input_file_path': s3_input_path\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a192c1-4557-4469-babf-d980a4d303c5",
   "metadata": {},
   "source": [
    "##  automation_to_datawarehouse_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908baf95-8b25-4c79-9923-3319237d637e",
   "metadata": {},
   "source": [
    "Below code is to automatically load processed log data i.e .parquet data to AWS RedShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4cc9a-9899-4318-b953-441f0b5f3d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Redshift Serverless configuration\n",
    "REDSHIFT_HOST = 'default-workgroup.914654949253.eu-north-1.redshift-serverless.amazonaws.com'\n",
    "REDSHIFT_PORT = '5439'\n",
    "REDSHIFT_DATABASE = 'careplus_db'  # Replace with your Redshift database name\n",
    "REDSHIFT_USER = 'admin'  # Replace with your Redshift username\n",
    "REDSHIFT_PASSWORD = 'RNBECghclb983*!'  # Replace with your Redshift password\n",
    "REDSHIFT_TABLE = 'public.support_logs'  # Replace with your table name\n",
    "IAM_ROLE = 'arn:aws:iam::914654949253:role/service-role/AmazonRedshift-CommandsAccessRole-20250922T002127'  # Your IAM role ARN\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # 1: read data from the bucket\n",
    "    # Get bucket and object key from the S3 event trigger\n",
    "    record = event['Records'][0]\n",
    "    bucket_name = record['s3']['bucket']['name']\n",
    "    input_key = record['s3']['object']['key']\n",
    "\n",
    "    print(f\"üì• Triggered by: s3://{bucket_name}/{input_key}\")\n",
    "\n",
    "    s3_input_path = f's3://{bucket_name}/{input_key}'\n",
    "\n",
    "    # Connect to Redshift Serverless using psycopg2\n",
    "    conn = psycopg2.connect(\n",
    "            host=REDSHIFT_HOST,\n",
    "            port=REDSHIFT_PORT,\n",
    "            dbname=REDSHIFT_DATABASE,\n",
    "            user=REDSHIFT_USER,\n",
    "            password=REDSHIFT_PASSWORD\n",
    "    )\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # COPY SQL query to load data from S3 into the Redshift table\n",
    "    copy_sql = f\"\"\"\n",
    "        COPY {REDSHIFT_TABLE}\n",
    "        FROM '{s3_input_path}'\n",
    "        IAM_ROLE '{IAM_ROLE}'\n",
    "        FORMAT AS PARQUET\n",
    "        REGION 'eu-north-1';\n",
    "        \"\"\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(copy_sql)\n",
    "\n",
    "    # Commit the changes (important for COPY operations)\n",
    "    conn.commit()\n",
    "\n",
    "    # Log success\n",
    "    print(f\"Data successfully copied from {s3_input_path} to {REDSHIFT_TABLE}\")\n",
    "\n",
    "\n",
    "    # Close the cursor and the connection\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca110ba-cc85-4252-9641-41e14737cf67",
   "metadata": {},
   "source": [
    "## automation_to_datawarehouse_tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521a9a4-22c7-40be-b1b5-f54df072561d",
   "metadata": {},
   "source": [
    "This code is to load processed data of tickets (.csv) files to AWS RedShift (datawarehouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b1d6ee-5a06-4de7-9079-88f4b3d7c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# --- Redshift Configuration ---\n",
    "REDSHIFT_HOST = os.environ['REDSHIFT_HOST']\n",
    "REDSHIFT_PORT = int(os.environ.get('REDSHIFT_PORT', 5439))\n",
    "REDSHIFT_DATABASE = os.environ['REDSHIFT_DATABASE']\n",
    "REDSHIFT_USER = os.environ['REDSHIFT_USER']\n",
    "REDSHIFT_PASSWORD = os.environ['REDSHIFT_PASSWORD']\n",
    "REDSHIFT_TABLE = os.environ['REDSHIFT_TABLE']\n",
    "IAM_ROLE = os.environ['IAM_ROLE']\n",
    "REGION = os.environ.get('AWS_REGION', '<your region>')\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # 1. Get S3 event details\n",
    "    record = event['Records'][0]\n",
    "    bucket_name = record['s3']['bucket']['name']\n",
    "    object_key = record['s3']['object']['key']\n",
    "\n",
    "    # Only process files that match our folder + CSV extension\n",
    "    if not object_key.startswith(\"support-tickets/processed/\") or not object_key.endswith(\".csv\"):\n",
    "        print(f\"Skipping file: s3://{bucket_name}/{object_key}\")\n",
    "        return {\"status\": \"skipped\"}\n",
    "\n",
    "    s3_path = f\"s3://{bucket_name}/{object_key}\"\n",
    "    print(f\"üì• New file detected: {s3_path}\")\n",
    "\n",
    "    # 2. Connect to Redshift\n",
    "    conn = psycopg2.connect(\n",
    "        host=REDSHIFT_HOST,\n",
    "        port=REDSHIFT_PORT,\n",
    "        dbname=REDSHIFT_DATABASE,\n",
    "        user=REDSHIFT_USER,\n",
    "        password=REDSHIFT_PASSWORD\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # 3. Build COPY command (CSV input)\n",
    "    copy_sql = f\"\"\"\n",
    "        COPY {REDSHIFT_TABLE}\n",
    "        FROM '{s3_path}'\n",
    "        IAM_ROLE '{IAM_ROLE}'\n",
    "        FORMAT AS CSV\n",
    "        IGNOREHEADER 1\n",
    "        REGION '{REGION}'\n",
    "        TIMEFORMAT 'auto';\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(copy_sql)\n",
    "        conn.commit()\n",
    "        print(f\"‚úÖ Data loaded successfully from {s3_path} into {REDSHIFT_TABLE}\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    return {\"status\": \"success\", \"file\": s3_path}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
